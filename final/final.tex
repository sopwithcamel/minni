%! program = pdflatex

%\documentclass[12pt,a4paper]{memoir} % for a long document
\documentclass[10pt,letter,final,article,twocolumn]{article} % for a short document
%\usepackage[left=0.25in,top=0.25in,right=0.25in,bottom=0.25in,nohead,nofoot]{geometry} 
\usepackage{titling,url}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[numbers]{natbib}
% See the ``Memoir customise'' template for some common customisations
% Don't forget to read the Memoir manual: memman.pdf

\newcommand{\rpc}[1]{\emph{#1}}

\title{Minni: Minimalistic MapReduce}
\author{Athula Balachandran \\
{\tt abalacha@cs.cmu.edu}
\and
Wolfgang Richter \\
{\tt wolf@cs.cmu.edu}
\and
Erik Zawadzki \\
{\tt epz@cs.cmu.edu}}
\date{May 7, 2010} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\pagenumbering{arabic}

\maketitle

\section{Introduction}
MapReduce~\citep{mapreduce08} is a distributed programming framework that facilitates the creation and execution of fault-tolerant data processing. Users concisely write their programs as two functions---the \emph{map} and \emph{reduce} functions---which the MapReduce run-time system distributes, schedules and executes on large clusters built with commodity hardware. 

There are many implementations of the MapReduce framework \citep{mochi,hadoop10,disco10,sphere09}. Of these, the most widely used is Hadoop. Hadoop was not designed to minimize memory use and as a result it has a large in-memory footprint. This prevents it from running on memory constrained systems, such as FAWN~\citep{fawn09}. Our project addresses this issue by identifying design decisions that are important to decreasing a MapReduce implementation's in-memory footprint. We implemented some of these design decisions to produce a light-weight MapReduce architecture that can be used by memory constrained systems.

%MapReduce~\citep{mapreduce08}, which was introduced by Google, supports distributed computing on huge datasets on large clusters involving commodity hardware. The framework allows programmers to easily write applications that process these datasets in a reliable and fault-tolerant manner.  The basic implementation  comprises of two stages---\emph{map} and \emph{reduce}. This framework is inspired by the map and reduce functions that are commonly used in functional programming. The input data set is split into independent chunks and are parallel processed by the \emph{map} tasks. The output from this stage is then passed---potentially after an optional local stage called \emph{combine}---to the \emph{reduce} tasks. The framework abstracts all details like scheduling tasks, monitoring tasks, and re-executing of failed tasks. 

%The architecture typically consists of a master node and several worker nodes that store data as well as execute both \emph{map} and \emph{reduce} jobs assigned by the master. The fact that the worker nodes store data as well as process them has been used to come up with efficient scheduling techniques that take into account locality and enable minimization of network traffic.

\section{Previous Work}

The foundation for our implementation is the original description of MapReduce~\citep{mapreduce08}. While MapReduce is one of the best known distributed computing frameworks, there are other related architectures such as Dryad~\citep{dryad07} and parallel relational databases (see, for example, \citet{db210} and \citet{teradata10}) which are much more general than MapReduce.  In this project we focus on the MapReduce framework, but our design decisions are also applicable for memory-constrained implementations of other frameworks.
%Our implementation on top of FAWN~\citep{fawn09} introduces a particular constraint that the original MapReduce design did not consider: low memory.  
%In addition, FAWN uses flash storage as its principal storage medium which introduces another area for MapReduce research---the original MapReduce design assumes a storage medium of hard drives.  

A recent paper by \citet{yu2009distributed} evaluates some of the different design choices for MapReduce and MapReduce-like systems. The key design decision they explore is the choice of aggregation strategy.  They show that the aggregation strategy used by Hadoop and the original MapReduce is empirically inferior to a competing method based on hashing which is used by major commercial databases (\emph{e.g.} DB2~\citep{db210} and Oracle~\citep{oracle10}). They also point out that the aggregation method has a significant impact on the memory footprint of jobs. This is particularly important for our project and we will discuss this issue later.

Other research has shown that techniques such as merging the MapReduce stages~\citep{barrier10}, and dynamically prioritizing resource usage~\citep{sandholm09} offer substantial performance benefits over the standard MapReduce design. We adopt both of these techniques in our implementation.

There are several existing MapReduce implementations, such as Disco~\citep{disco10}, Hadoop~\citep{hadoop10}, and Sector and Sphere~\citep{sphere09}. These implementations differ from our own since they do not target low-memory, but we did use these other implementations---Hadoop in particular---as a guide to identify important design decisions. In addition to these general MapReduce implementations there are also a number of hardware-specific implementations that target the Cell Broadband architecture~\citep{rafique09} and multicore systems~\citep{chu06}. These implementations have an orthogonal goal to ours: we want to handle large arrays of heterogenous computers that may have limited memory.

%Finally, we can take advantage of FAWN's energy efficiency which is an active area of research with MapReduce as reflected in Gordon~\citep{gordon09},  and work analyzing MapReduce traces for future energy efficiency goals~\citep{chen10}.

\section{Architecture}
\label{sec:arch}

Our proposed architecture is shown in Figure~\ref{fig:arch} and the internal design of the master is exposed in Figure~\ref{fig:master}. We first describe the major differences between our proposed architecture and the architecture suggested in prior implementations of MapReduce. We then explain our architecture and the RPCs it uses to communicate by taking the reader through the running of a typical MapReduce job. We finish this section by describing our mechanisms for tolerating faults and summarize the RPCs used in our architecture.

\begin{figure}[htbp]
\begin{center}
\resizebox{\columnwidth}{!}{
\includegraphics{Architecture.pdf}
}
\caption{The major components found in our architecture.}
\label{fig:arch}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\resizebox{\columnwidth}{!}{
\includegraphics{master.pdf}
}
\caption{A diagram of the master system and the components that it communicates with.}
\label{fig:master}
\end{center}
\end{figure}

\subsection{Design Decisions}

There are three major differences between our implementation and standard MapReduce. The first is the introduction of a local manager process called the \emph{WorkDaemon} (WD) that manages a single node. The second difference is that we use the \emph{Accumulator-PartialHash} (APH) aggregation scheme, as described in \citet{yu2009distributed}.  The third is the ability to start reducer jobs immediately after a map job finishes.

\subsubsection{WorkDaemon}

The first major difference is the WD that sits on each node that mediates all communication between the master and the worker---including RPCing intermediate key/values to the reducers. This has some advantages and disadvantages.

There are three advantages to this approach. The first is that mapper threads can free system resources by closing immediately upon completion. This is especially important in systems where memory is scarce. This property has a secondary advantage: it entirely removes a failure case. Jobs no longer need to be rerun if mappers crash after completion since the daemon is responsible for transferring data from mappers to reducers. Additionally, since worker communication is centralized by the WD we can reduce network overhead by batching messages.

The principle disadvantage of the WD approach is that the WD takes the node down when it crashes. In our failure model a node is removed from the MapReduce pool if the WD crashes. However, since the WD is isolated from the user's code and data, we expect this to be a somewhat rare event.

\subsubsection{Accumulator-PartialHash}

The second major difference is the adoption of the APH aggregation scheme. \citet{yu2009distributed} describes the original MapReduce approach as the \emph{FullSort} method: all the records are gathered in memory and sorted by intermediate key immediately following the map phase\footnote{If there are too many key-value pairs to store in memory then they are sorted externally.}. This method has two principle disadvantages: it may be memory intensive (unless we always sort externally in which case it is slow), and requires every key-value pair to be present in memory before sorting. 

 APH aggregation alleviates both these problems. APH essentially combines the sort phase into an initial reduction. Each task has a hash table that maps keys to partial aggregation objects (PAOs).These PAOs, intuitively, are some sufficient description of an incomplete aggregation such that:
\begin{itemize}
 \item a new key-value pair can be \emph{added} to the PAO to yield a new PAO (\emph{i.e.} $<k,p^t> = <k,p^{t-1}> + <k,v^t>$);
 \item two PAOs can be \emph{merged} to form a new PAO (\emph{i.e.} $<k,p^s> = <k,p^q> \oplus <k,p^r>$).
\end{itemize}
When a new object (either a value or a PAO) with a particular key is given to a reducer task, the task either adds it to or merges it with the appropriate PAO in the hash table. The PAO is equivalent to a suspended reduce task. The user describes this reduction with five functions:

\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
\item {Initialize}
\item {Add}
\item {Merge}
\item {Serialize}\footnote{In a future version of our library we plan to provide serialization support for standard STL containers by default.}
\item {Deserialize}
\end{enumerate}

This new formulation yields some immediate advantages: if the PAO grows less than linearly in the number of key-value pairs then the memory used by a collection of PAOs will be less than the memory used by the complete list of key-value pairs. Indeed, for many examples like $k^{th}$-order statistics and word count, the PAO is constant size. In other cases like grep the size of the PAO---represented internally by, say, a list of lines that match the pattern---might actually grow linearly with the size of the input but with a very small constant.

Additionally, by evicting PAO objects that are either too large or cause many collisions in the hash table, we can guarantee an upper bound on the memory footprint for a reducer task. The \emph{eviction policy}---the policy that selects PAOs to write to disk---must balance between evicting early to save memory and evicting late to maximize data compression.  

Another advantage of this approach is that we can start reducers immediately after at least one mapper finishes. This eliminates time spent waiting for key-value pairs to gather. Finally, since we don't sort, APH aggregation only requires key equality tests, rather than order comparisons. This means the programmer can avoid having to define spurious ``greater-than'' notions.

Finally, the use of PAOs suggests useful design modifications for the mapper. In the traditional MapReduce system, each mapper writes intermediate key-value pairs to local disk. These key-value pairs might be shipped to the reducers as-is or merged prior to transmission in a combiner phase. The combiner phase is a local reduce that reduces the size of the transmitted data. In our scheme, rather than writing the intermediate key-values to files, we have a \emph{per-mapper local reducer} that aggregates the key-value pairs into PAOs. These PAOs are then written to disk, and may be combined by a \emph{per-node local reducer}.

This achieves the same end result as the original combiner phase, but may be better in terms of memory, performance and disk usage. The PAOs that result from the \emph{per-mapper local reducer} are generally much smaller than the total list of key-value pairs which makes them more manageable for keeping in memory, rather than buffering and writing key-value pairs to disk.

Our approach also has disadvantages. Chief among these, some users may have a MapReduce formulation for their problem which assumes that the key-value pairs are initially sorted and rely on this fact to process them in sequence. We break this assumption in our prototype and explicitly assume that key $k_i$ may be processed entirely independently from the processing of key $k_j$. In a full-featured release version of this program we would likely want to provide the user with a way of insisting that the key-value pairs be sorted, but we will not initially include this feature.

Early in the design phase we thought that the PAOs would obfuscate errors. For example, in the original MapReduce paper, key-value pairs could be flagged as `bad' and skipped if multiple reduce or map tasks fail on them. While we can still mark a record that causes repeated problems when we try to add it to a PAO as `bad,' it is less clear what should be done if merging PAOs fails. Scrapping work and rerunning the tasks that generated the PAO could solve the problem if there was an order-dependent error, but what do we do if PAOs fail to merge multiple times?

We believe that  the original intent of the bad-records was primarily to guard against anomalous data, and not logic problems with the user code. We can still provide reliability in the face of anomalous data by recording the last key-value pair that was successfully added to a PAO in the initial reduce, and some robustness to logical errors by rerunning PAO calculations, but we will  pass any repeated merger errors to the user---such errors represents a serious logical problem that is likely independent from \emph{where} the task was run. Additionally, if we have consistent problems merging two user-defined objects, this casts serious doubt on the validity of the user's code.

We feel that our design decision to use APH aggregation is reasonable for a MapReduce implementation that is primarily intended for memory-constrained systems.

\subsection{Typical Use}

\begin{figure}[htbp]
\begin{center}
\resizebox{\columnwidth}{!}{
\includegraphics{data_flow.pdf}
}
\caption{An example of how data flows through our system.}
\label{fig:flow}
\end{center}
\end{figure}

So how does a typical MapReduce job look on this system? The sequence is initiated by the user when he contacts the master process and submits a MapReduce job using the \rpc{SubmitJob} RPC, which the user will block on.  In the initial version of our system we assume that there is a single well-known master that is always available and that failures of the master are catastrophic. The details for the job are provided in the form of a \emph{specification object}. The user provides in the specification the name of the shared object library that contains the user code (the \emph{map} function and all the PAO operations), the location on the DFS of the input data, and the DFS location to write all the output. The user passes this code to our program by implementing known functions and dynamically linking their object files into our code.

\begin{table}[htdp]
\begin{center}
{\footnotesize
\begin{tabular}{|l|l|}\hline\setlength{\tabcolsep}{1pt}
\label{tab:rpc}
\textbf{RPC} & \textbf{Purpose}\\
\hline
\rpc{Kill} & Stop a running task\\
\rpc{SendData} & Send data from mapper to reducer\\
\rpc{SubmitJob} & Start MapReduce job\\
\rpc{StartMapper} & Run a map task \\
\rpc{StartReducer} & Run a reduce task\\
\rpc{ReportData} & Notification of finished map tasks\\
\rpc{ReportWorkComplete} & Poll for task status\\
\rpc{ReportFail} & Notify reducers of failed maps\\\hline
\end{tabular}}
\end{center}
\caption{Overview of select RPCs in Minni.}
\end{table}

After receiving the initial message the master requests the WorkDaemon on each node to start \emph{mapper} threads with a \rpc{StartMapper} call. The master prefers placing \emph{map} jobs on nodes that have a replicated copy of the appropriate split of the input data and nodes that are not busy.  We assume in this phase of the work that the user has access to the complete cluster and that the current MapReduce job is the only job running on the system. Later iterations of our work will have to include interactions with a cluster resource manager. We also currently assume that the WDs are already running on the worker nodes.

The WD, upon receipt of a \emph{map} work request, spawns a new \emph{mapper} task for each request it gets. Because the WD never delays or rejects any work request the master is completely responsible for work scheduling (the master maintains a tentative queue for each WD). The \rpc{StartMapper} call includes a chunk ID for the DFS. For now, we assume that each chunk belongs to a single file and that we do not have to worry about the boundaries of multi-chunk files---\textit{e.g.} if a text document spans two chunks a word does not start in one block and end in another.

While mapping the input key-value pairs the mapper task also performs a  \emph{per-mapper local reduction}. To do this the mapper maintains a hash table of PAOs---one for each partition. When an input key-value pair has been mapped to an intermediate key-value the mapper task adds it to the appropriate PAO. Upon termination of the mapper the resulting PAOs are written to local disk and the mapper task informs the WD that it is finished and passes back a list of files it created. The WD can then start an optional \emph{per-node local reducer}. The master occasionally polls the WDs for status changes, and upon receipt of one of these RPCs the WD will list all finished (or dead) mappers.

When it learns that some mappers are finished, the master initiates new \emph{final reducer} tasks with a \rpc{StartReducer} call. Whenever a WD informs the master that it has some new PAOs to read, the master passes this information on to the appropriate \emph{final reducers} with a \rpc{ReportData} call. If the PAOs  are local then the \emph{final reducer} can read them right from disk. If not, then the key/values are requested directly from the remote WD with a \rpc{SendData} call. The WD is responsible for tracking the state of all transmissions and collecting the required PAOs for its final reducers. The \emph{final reducer} starts executing the user's \emph{merge} code as soon as it has collected at least two PAOs.

While running the user's code the \emph{per-mapper local reducer} thread writes the intermediate key-PAO pairs to a temporary local file. The \emph{final reducer} aggregates and writes the final key-value pairs to the DFS and reports to the WD. The WD forwards this to the master when polled by the \rpc{ReportWorkComplete} call.

We have summarized this data flow in Figure~\ref{fig:flow}.

\subsection{Fault Tolerance}
The above process must be resilient to machines and threads failing. We tolerate worker nodes, worker tasks, and WorkDaemons crashing. We do not tolerate the master failing.

We use a polling model to keep track of the dispatched jobs. The master polls WDs to see if any of the active jobs have finished or failed. Whenever a mapper or a reducer task is working, the WD checks its running status to ensure that it has not terminated before reporting that it has completed its work.

If a job dies unexpectedly then the master will reschedule it. Notice that because the WD is responsible for sending data to the \emph{final reducers}, we do not need to notify any \emph{final reducers} about failed \emph{mappers}. 

If the entire node goes down (\textit{i.e.} the WD has not checked in for a while), then the master sends a \rpc{Kill} to the WD for all of its jobs, then \rpc{ReportFail}'s any reducers that are still waiting for PAOs from that node. Notice that since we ensure that all or no PAOs are merged by a \emph{final reducer}, a node crash cannot leave us with a case where we have used some but not all of a \emph{per-mapper local reducer's} PAOs. Finally, the master reschedules all work on the downed node.

\subsection{Implementation Details}

Minni is a C++ MapReduce implementation with both internal and external design changes based on modern research literature and the need to operate in a low memory environment.  We used Thrift~\citep{thrift10}---an open-source RPC library provided by the Apache Foundation---and the Thread Building Blocks~\citep{intel10} library---an open-source threading library provided by Intel.  These two libraries enabled rapid development and provided a tested
foundation upon which Minni has been constructed.  In addition, support for both Hadoop Distributed File System~\citep{hadoop10} and KosmosFS~\citep{kfs10} has been provided in the implementation of Minni.

The API of Minni closely follows that of the original MapReduce, deviating only in the Reduce phase through the use of PAOs.  This provides developers with a familiar environment and makes porting existing MapReduce applications easy.

%\subsection{RPC calls}
%
%Here is a summary of the RPC calls used in our system:
%
%\begin{table}[htdp]
%\caption{Stored procedures on the master.}
%\begin{center}
%\begin{tabular}{|l|l|}\hline
%\textbf{Procedure} & \textbf{Arguments}\\\hline
%\rpc{SubmitJob} & SpecObject\\\hline
%\end{tabular}
%\end{center}
%\label{tab:master_rpc}
%\end{table}%
%
%\begin{table}[htdp]
%\caption{Stored procedures on the worker daemon}
%\begin{center}
%\begin{tabular}{|l|l|}\hline
%\textbf{Procedure} & \textbf{Arguments}\\\hline
%\rpc{StartMapper} & JobID, JobProperties\\
%\rpc{StartReducer} & JobID,  JobProperties\\
%\rpc{ReportData} & \\
%\rpc{ReportFail} & Mapper IP, Mapper JobID\\
%\rpc{SendData} & JobID, Key\\\hline
%\end{tabular}
%\end{center}
%\label{tab:worker_rpc}
%\end{table}%

\section{Evaluation}
In our evaluation section we are primarily interested in two things: first, do we actually have a smaller in-memory footprint than Hadoop? Secondly, how quickly do similar MapReduce jobs take Hadoop and Minni?

\subsection{Benchmark Setup}
We ran all of our tests on a single node simulating near serialization of the 
MapReduce job.  This is a worst case scenario for MapReduce.  The node has an Intel Core2 Duo CPU P8600 @ 2.40GHz,
and 4 GB of RAM.  The distributed file system we used was HDFS in order to give comparable results between Hadoop
and Minni.  We created a 10 GB dataset by concatenating ASCII representations of integers uniformly distributed in the
range $[1000,2000]$ separated by a space.  The benchmarking application of choice was WordCount because an implementation
already exists for Hadoop and coding WordCount in Minni was simple.

We also worked on $8$ Dell servers provided by Satya.  We didn't have time to do experiments on these machines to investigate
scaling, but we were able to verify that a multiple node Minni system works.  In addition, Vijay Vasudevan granted us access to a 
FAWN node which we will use to verify that Minni executes on FAWN.

\subsection{Memory Profiling and Performance}

\begin{figure*}
\centering
\subfloat[Hadoop WordCount]{\includegraphics[width=0.5\textwidth]{hadoop_trace.pdf}}
\subfloat[Minni WordCount]{\includegraphics[width=0.425\textwidth]{minni_trace.pdf}}
%\vspace{-0.1in}
\caption{Comparison of memory usage in Hadoop and Minni, using 1 GB dataset.}
\label{fig:memory}
%\vspace*{.2in}
\end{figure*}

For profiling memory we used a bash wrapper script around {\tt ps} to query
running process virtual memory and resident memory statistics.  Our polling
rate was $10$ seconds, and we present the results in Figure~\ref{fig:memory}.
The plots show that Minni is not efficient with its use of memory.  In this case, the
inefficiency arises from our experimental setup in using HDFS.  The interface that
Minni uses is a JNI-based C++ interface which means that Minni runs an embedded
JVM every time it accesses HDFS.  We conjecture that this JVM is responsible for most of the memory usage.
Since the embedded JVM is difficult to profile with tools such as Valgrind, we didn't have time to investigate further.

We know that Minni starts up quicker and with less memory than Hadoop---about $10$ times less memory (Hadoop's
TaskTracker starts with $~40$MB of resident memory and Minni's WorkDaemon starts with ~$4$MB).  Once calls to HDFS start, memory
usage increases greatly.

\section{Future Work}
We need to address multiple issues to make Minni a usable library.  We must:
\begin{itemize}
\item Implement robust fault-tolerance
\item Optimize memory usage
\item Fix open bugs
\item Implement a logging framework
\item Provide more abstractions in the user library
\item Expose more tunable parameters
\item Make external data sources such as databases accessible
\end{itemize}

In addition, we plan to explore:
\begin{itemize}
\item Dryad-Like functionality
\item Optimizations discussed by other project groups this semester
\item External sorting within our system
\end{itemize}

Within only one semester, providing a fully polished library was not feasible.  What
we provide today is a working library that can run MapReduce jobs and interface with
distributed file systems.  We explored the design space of MapReduce, created a preliminary
library, and identified the work needed to make our library usable by a larger community.

\section{Conclusion}
We had a lot of fun working on this project, and would like to thank both of you guys (Dave+Iulian)
for your direction and help with the project.  Implementing MapReduce was more work than we had planned, but
we learned a lot along the way.  With more polish and optimization, we think Minni will be a good choice for memory-constrained
clusters.  Have a great summer!

\bibliographystyle{plainnat}
{\small
\bibliography{references}
}
\end{document}
